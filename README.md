# ğŸ§  Toxic Comment Detection using Logistic Regression & TF-IDF

This project classifies whether a comment contains **toxic, threatening, or hateful language** using classic NLP + machine learning:

- ğŸ”¹ Traditional ML: **TF-IDF + Logistic Regression**
- ğŸ§  Outputs **natural-language explanation**
- ğŸ–¥ï¸ Clean & responsive UI built with **Streamlit**

---


## ğŸ“¸ Screenshots

Visual demonstration of comment classification results:

| âœ… Clean Comment | âŒ Toxic Comment |
|------------------|-----------------|
| ![Clean](https://via.placeholder.com/350x200?text=Clean+Prediction) | ![Toxic](https://via.placeholder.com/350x200?text=Toxic+Prediction) |

---

## ğŸ” Features

- ğŸ§  Logistic Regression with TF-IDF (multi-label classification)
- ğŸ’¬ Predict 6 toxicity types in 1 go
- ğŸª„ Smart interpretation like:
  > â€œThis comment appears to be: **Toxic and Hateful**â€
- ğŸ“Š Confusion matrix & metrics
- âš–ï¸ Handles imbalanced data
- ğŸ–¼ï¸ Streamlit web UI included

---

## ğŸ·ï¸ Labels Detected

| Label          | Meaning                            |
|----------------|-------------------------------------|
| `toxic`        | Mildly offensive                    |
| `severe_toxic` | Extremely abusive                   |
| `obscene`      | Obscene or vulgar language          |
| `threat`       | Threats of violence or harm         |
| `insult`       | Personal attacks or mockery         |
| `identity_hate`| Hate speech against identity groups |

---

## ğŸ§  Model Details

### âœ… TF-IDF + Logistic Regression
- Uses `TfidfVectorizer` with unigrams & bigrams
- One model per label (multi-label)
- Optimized with `class_weight='balanced'`
- Thresholding applied for label confidence
- Fully interpretable

---

## ğŸ§ª Test Examples

| Comment                                                              | Predicted Labels                     |
|----------------------------------------------------------------------|--------------------------------------|
| "Youâ€™re such a dumb loser."                                          | `toxic`, `insult`                    |
| "F*** you and your stupid face."                                     | `toxic`, `obscene`                   |
| "Iâ€™ll find you and break your legs."                                 | `toxic`, `threat`                    |
| "Thanks for your help, I really appreciate it."                      | _(None - clean)_                     |
| "People from your race are filth."                                   | `identity_hate`, `toxic`             |

---

## ğŸ“¥ Dataset

Used the official **Jigsaw Toxic Comment Classification Challenge** dataset.

ğŸ“ [Download from Kaggle](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data)

---

## ğŸ› ï¸ Tech Stack

| Layer         | Tools / Libraries                           |
|---------------|----------------------------------------------|
| Language      | Python 3.x                                   |
| UI            | Streamlit                                    |
| NLP           | NLTK, Scikit-learn                           |
| Vectorization | TF-IDF                                       |
| Model         | Logistic Regression                          |
| Deployment    | Streamlit Cloud / Local                      |



---

## ğŸ”® Future Enhancements

- ğŸ¤– Add Random Forest & XGBoost comparisons
- ğŸ§¬ Transformer-based classifier (BERT)
- ğŸ” Model interpretability with SHAP
- ğŸ›¡ï¸ Toxicity severity score
- ğŸ§ª Batch CSV prediction upload
- â˜ï¸ Cloud deployment (Streamlit Cloud or Hugging Face)

---

## ğŸ“„ License

This project is licensed under the [MIT License](https://opensource.org/licenses/MIT).  
Feel free to use, modify, and distribute with credit.

---

## ğŸ™‹â€â™‚ï¸ Author

**Dhruvin Patel**  
ğŸ“ B.Tech â€“ Computer Science  
ğŸ“ Gujarat, India  
ğŸ“§ Email: pateldhruvin6122@gmail.com  
ğŸ”— GitHub: [@dhruvin6122](https://github.com/dhruvin6122)  
ğŸ’¼ LinkedIn: [View Profile](https://www.linkedin.com/in/patel-dhruvin-70b602280)
